# workflow_engine/core/values/schema.py
"""
Pydantic models that cover the subset of the JSON Schema language generated by
Pydantic for Value subclasses.
"""

from __future__ import annotations

from collections.abc import Mapping, Sequence
from functools import cached_property
from typing import Any, ClassVar, Literal

from overrides import override
from pydantic import ConfigDict, Field, ValidationError, field_validator

from ...utils.immutable import ImmutableBaseModel, ImmutableRootModel
from .data import Data, DataValue, build_data_type
from .mapping import StringMapValue
from .primitives import (
    BooleanValue,
    FloatValue,
    IntegerValue,
    NullValue,
    StringValue,
)
from .sequence import SequenceValue
from .value import Value, ValueType, value_type_registry


def merge_defs(
    a: Mapping[str, ValueSchema] | None,
    b: Mapping[str, ValueSchema] | None,
) -> Mapping[str, ValueSchema] | None:
    if a is None:
        return {} if b is None else b
    if b is None:
        return {} if a is None else a
    return dict(**a, **b)


class BaseValueSchema(ImmutableBaseModel):
    # We will only handle fields that Pydantic actually uses.
    # extra="forbid" offensively validates that we haven't missed any fields.
    model_config: ClassVar[ConfigDict] = ConfigDict(extra="forbid")

    defs: Mapping[str, ValueSchema] = Field(default_factory=dict, alias="$defs")
    title: str | None = None
    description: str | None = None
    default: Any | None = None

    # HACK: fixes a bug where the ValueSchemas in defs are not recursively validated
    @field_validator("defs", mode="after")
    @classmethod
    def fix_defs(
        cls,
        defs: Mapping[str, ValueSchema],
    ) -> Mapping[str, ValueSchema]:
        return {k: validate_value_schema(schema) for k, schema in defs.items()}

    def to_value_cls(
        self,
        *extra_defs: Mapping[str, ValueSchema],
    ) -> ValueType:
        """
        Resolves this schema to a Pydantic class.
        If the title matches a concrete Value class, then it returns that class
        right away, regardless the other contents of the schema.
        This is technically a hack, but it is a massive quality of life boost
        for users, who can just provide { "title": "CustomValue" } as an alias
        for the entire CustomValue schema.

        References, if any, are resolved using self.defs first, then any
        extra_defs in order of decreasing precedence.
        """
        if self.title is not None and self.title in value_type_registry:
            return value_type_registry[self.title]
        return self.build_value_cls(*extra_defs)

    def build_value_cls(
        self,
        *extra_defs: Mapping[str, ValueSchema],
    ) -> ValueType:
        """
        Builds a Pydantic class from this schema. References, if any, are
        resolved using self.defs first, then any extra_defs in order of decreasing precedence.
        """
        raise NotImplementedError("Subclasses must implement this method")


class BooleanValueSchema(BaseValueSchema):
    type: Literal["boolean"]

    @override
    def build_value_cls(
        self,
        *extra_defs: Mapping[str, ValueSchema],
    ) -> type[BooleanValue]:
        return BooleanValue


class NullValueSchema(BaseValueSchema):
    type: Literal["null"]

    @override
    def build_value_cls(
        self,
        *extra_defs: Mapping[str, ValueSchema],
    ) -> type[NullValue]:
        return NullValue


class IntegerValueSchema(BaseValueSchema):
    # unused JSON Schema fields include minimum, maximum, multipleOf, etc.
    type: Literal["integer"]

    @override
    def build_value_cls(
        self,
        *extra_defs: Mapping[str, ValueSchema],
    ) -> type[IntegerValue]:
        return IntegerValue


class FloatValueSchema(BaseValueSchema):
    # unused JSON Schema fields include minimum, maximum, multipleOf, etc.
    type: Literal["number"]

    @override
    def build_value_cls(
        self,
        *extra_defs: Mapping[str, ValueSchema],
    ) -> type[FloatValue]:
        return FloatValue


class StringValueSchema(BaseValueSchema):
    # unused JSON Schema fields include length and pattern
    type: Literal["string"]

    @override
    def build_value_cls(
        self,
        *extra_defs: Mapping[str, ValueSchema],
    ) -> type[StringValue]:
        return StringValue


class SequenceValueSchema(BaseValueSchema):
    type: Literal["array"]
    items: ValueSchema

    @override
    def build_value_cls(
        self,
        *extra_defs: Mapping[str, ValueSchema],
    ) -> type[SequenceValue]:
        T = self.items.to_value_cls(self.defs, *extra_defs)
        return SequenceValue[T]


class StringMapValueSchema(BaseValueSchema):
    type: Literal["object"]
    additionalProperties: ValueSchema | Literal[True] = True

    @override
    def build_value_cls(
        self,
        *extra_defs: Mapping[str, ValueSchema],
    ) -> type[StringMapValue]:
        if self.additionalProperties is True:
            return StringMapValue[Value]
        else:
            V = self.additionalProperties.to_value_cls(self.defs, *extra_defs)
            return StringMapValue[V]


class DataValueSchema(BaseValueSchema):
    type: Literal["object"]
    properties: Mapping[str, ValueSchema]
    additionalProperties: ValueSchema | bool = False
    required: Sequence[str] = Field(default_factory=list)

    # HACK: fixes a bug where the ValueSchemaReferences in properties are not recursively validated
    @field_validator("properties", mode="after")
    @classmethod
    def fix_defs(
        cls,
        properties: Mapping[str, ValueSchema],
    ) -> Mapping[str, ValueSchema]:
        return {k: validate_value_schema(schema) for k, schema in properties.items()}

    def build_data_cls(
        self,
        *extra_defs: Mapping[str, ValueSchema],
    ) -> type[Data]:
        required_set = frozenset(self.required)
        properties = {
            k: (
                v.to_value_cls(self.defs, *extra_defs),
                k in required_set,
            )
            for k, v in self.properties.items()
        }
        assert self.title is not None
        return build_data_type(self.title, properties)

    @override
    def build_value_cls(
        self,
        *extra_defs: Mapping[str, ValueSchema],
    ) -> type[DataValue]:
        D = self.build_data_cls(*extra_defs)
        return DataValue[D]


class ReferenceValueSchema(BaseValueSchema):
    """
    A schema of the form
    { "$defs": { "foo": ... }, "$ref": "#/$defs/foo" }
    """

    ref: str = Field(alias="$ref", pattern=r"^#/\$defs/\w+$")

    @cached_property
    def id(self) -> str:
        _hash, _defs, id = self.ref.split("/")
        assert _hash == "#"
        assert _defs == "$defs"
        return id

    @override
    def build_value_cls(
        self,
        *extra_defs: Mapping[str, ValueSchema],
    ) -> ValueType:
        for defs in (self.defs, *extra_defs):
            if self.id in defs:
                return defs[self.id].to_value_cls(self.defs, *extra_defs)
        raise KeyError(f"Schema definition for {self.id} not found")


# TODO: add proper union support in the future. Disabled until then.
# class UnionValueSchema(BaseValueSchema):
#     anyOf: Sequence[ValueSchema]


type ValueSchema = (
    BooleanValueSchema
    | DataValueSchema
    | FloatValueSchema
    | IntegerValueSchema
    | NullValueSchema
    | SequenceValueSchema
    | StringMapValueSchema
    | StringValueSchema
    # | UnionValueSchema
    | ReferenceValueSchema  # must be handled second-to-last
    | BaseValueSchema  # must be handled last
)


ValueSchemaRootModel = ImmutableRootModel[ValueSchema]


def validate_value_schema(schema: Any) -> ValueSchema:
    try:
        return ValueSchemaRootModel.model_validate(schema).root
    except ValidationError as e:
        raise ValueError(f"Invalid value schema: {schema}") from e


__all__ = [
    "ValueSchema",
]
